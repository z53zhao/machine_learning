{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Your-name:\" data-toc-modified-id=\"Your-name:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Your name:</a></span></li><li><span><a href=\"#Collaborators:\" data-toc-modified-id=\"Collaborators:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Collaborators:</a></span></li><li><span><a href=\"#TensorFlow\" data-toc-modified-id=\"TensorFlow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TensorFlow</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your name:\n",
    "\n",
    "<pre> Your name here</pre>\n",
    "\n",
    "### Collaborators:\n",
    "\n",
    "<pre> Enter the name of the people you worked with if any</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. When is a variable initialized? When is it destroyed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At the beginning of a seession. End of session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the difference between a placeholder and a variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You use a variable when you need to store the state of a graph and a placeholder when you want to input some external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. Train it and evaluate it on the moons dataset (introduced in Chapter 5). Try adding all the bells and whistles:\n",
    "\n",
    "- Define the graph within a logistic_regression() function that can be reused easily.\n",
    "\n",
    "- Save checkpoints using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "\n",
    "- Restore the last checkpoint upon startup if training was interrupted.\n",
    "\n",
    "- Define the graph using name scopes so the graph looks good in TensorBoard.\n",
    "\n",
    "- Add summaries to visualize the learning curves in TensorBoard.\n",
    "\n",
    "- Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)\n",
    "\n",
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]\n",
    "y_moons_column_vector = y_moons.reshape(-1, 1)\n",
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]\n",
    "\n",
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\" )\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary =tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix +=\"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 6\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.62998503\n",
      "Epoch: 500 \tLoss: 0.16122366\n",
      "Epoch: 1000 \tLoss: 0.1190321\n",
      "Epoch: 1500 \tLoss: 0.097329214\n",
      "Epoch: 2000 \tLoss: 0.08369793\n",
      "Epoch: 2500 \tLoss: 0.07437582\n",
      "Epoch: 3000 \tLoss: 0.06750215\n",
      "Epoch: 3500 \tLoss: 0.062206898\n",
      "Epoch: 4000 \tLoss: 0.058026787\n",
      "Epoch: 4500 \tLoss: 0.05456297\n",
      "Epoch: 5000 \tLoss: 0.051708277\n",
      "Epoch: 5500 \tLoss: 0.04923773\n",
      "Epoch: 6000 \tLoss: 0.047167283\n",
      "Epoch: 6500 \tLoss: 0.045376644\n",
      "Epoch: 7000 \tLoss: 0.04381875\n",
      "Epoch: 7500 \tLoss: 0.042374235\n",
      "Epoch: 8000 \tLoss: 0.041089173\n",
      "Epoch: 8500 \tLoss: 0.039970923\n",
      "Epoch: 9000 \tLoss: 0.038920265\n",
      "Epoch: 9500 \tLoss: 0.038010757\n",
      "Epoch: 10000 \tLoss: 0.037155706\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path+\".epoch\"\n",
    "final_model_path = \"./logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):     \n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\" Training was interrupted. Check if the checkpoint file exists, restore the model and load the epoch number. Continue the epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y:y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        \n",
    "        if epoch %500 ==0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, 'wb') as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "    \n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "print(\"precision_score\", precision_score(y_test, y_pred))\n",
    "print(\"recall_score\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  logdir: tf_logs/logreg-run-20190415160751/\n",
      "  batch size: 54\n",
      "  learning_rate: 0.004430375245218265\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 1\n",
      "  logdir: tf_logs/logreg-run-20190415160900/\n",
      "  batch size: 22\n",
      "  learning_rate: 0.0017826497151386947\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 2\n",
      "  logdir: tf_logs/logreg-run-20190415161123/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.00203228544324115\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 3\n",
      "  logdir: tf_logs/logreg-run-20190415161210/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.004491523825137997\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 4\n",
      "  logdir: tf_logs/logreg-run-20190415161310/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.07963234721775589\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 5\n",
      "  logdir: tf_logs/logreg-run-20190415161414/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.0004634250583294876\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 6\n",
      "  logdir: tf_logs/logreg-run-20190415161457/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.047706818419354494\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 7\n",
      "  logdir: tf_logs/logreg-run-20190415161552/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.0001694044709524274\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 8\n",
      "  logdir: tf_logs/logreg-run-20190415161656/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.04171461199412461\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n",
      "Iteration 9\n",
      "  logdir: tf_logs/logreg-run-20190415161758/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.00010742922968438615\n",
      "  training: .....................precision_score 0.9797979797979798\n",
      "recall_score 0.9797979797979798\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations =10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1,100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs =6\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs+1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "    final_model_path = \"./logreg_model%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y:y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 ==0:\n",
    "                print(\".\",end=\"\")                \n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred =(y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"precision_score\", precision_score(y_test, y_pred))\n",
    "        print(\"recall_score\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Similar to the linear regression implementation in class, write a lasso regression implementation. Use the same dataset, and choose a value for the penalty $\\alpha$:\n",
    "\n",
    "Using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "\n",
    "Restore the last checkpoint upon startup if training was interrupted.\n",
    "\n",
    "Define the graph using name scopes so the graph looks good in TensorBoard.\n",
    "\n",
    "Add summaries to visualize the learning curves in TensorBoard.\n",
    "\n",
    "Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_lasso(X, y, initializer=None, seed=42, learning_rate=0.01, lasso=0.9):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"linear_regression_lasso\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "    \n",
    "            A = tf.Variable(tf.random_normal([7, 1], seed=seed), name=\"A\")\n",
    "            b = tf.Variable(tf.random_normal([1, 1], seed=seed), name=\"b\")\n",
    "            y_pred = tf.add(tf.matmul(X, A), b)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            lasso_param = tf.constant(0.9)\n",
    "            heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50.,tf.add(A, -lasso_param)))))\n",
    "            regularization_param = tf.multiply(heavyside_step, 99.)\n",
    "            loss = tf.add(tf.reduce_mean(tf.square(y - y_pred)), regularization_param)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary =tf.summary.scalar('lasso_loss', tf.reduce_mean(loss, name = 'loss'))\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_pred, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 6\n",
    "logdir = log_dir(\"lasso\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_pred, loss, training_op, loss_summary, init, saver = linear_regression_lasso(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index in range(1):\n",
    "    X_batch_1, y_batch_1 = random_batch(X_train_enhanced, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[122.052124],\n",
      "       [ 23.076387],\n",
      "       [ 23.076387],\n",
      "       [ 23.076387],\n",
      "       [ 23.076387],\n",
      "       [ 23.076387],\n",
      "       [ 23.076433]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #sess.run(training_op, feed_dict={X: X_train_enhanced, y:y_train})\n",
    "\n",
    "\n",
    "        \n",
    "    sess.run(training_op, feed_dict={X: X_batch_1, y:y_batch_1})    \n",
    "    loss_val = sess.run([loss], feed_dict={X: X_test_enhanced, y:y_test})    \n",
    "    y_pred_val = y_pred.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    print(loss_val)\n",
    "    #sess.run(training_op, feed_dict={X: X_batch_1, y:y_batch_1})    \n",
    "    #loss_val = sess.run([loss], feed_dict={X: X_test_enhanced, y:y_test})    \n",
    "    #y_pred_val = y_pred.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    #print(loss_val)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "Epoch: 0 \tLoss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/lasso_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path+\".epoch\"\n",
    "final_model_path = \"./lasso_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):     \n",
    "        #with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "        #    start_epoch = int(f.read())\n",
    "        #print(\" Training was interrupted. Check if the checkpoint file exists, restore the model and load the epoch number. Continue the epoch\", start_epoch)\n",
    "        #saver.restore(sess, checkpoint_path)\n",
    "        sess.run(init)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y:y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        print(loss_val)\n",
    "        \n",
    "        if epoch %500 ==0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, 'wb') as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "    \n",
    "    saver.save(sess, final_model_path)\n",
    "    y_pred_val = y_pred.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations =10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1,100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs =6\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs+1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "    final_model_path = \"./logreg_model%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y:y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 ==0:\n",
    "                print(\".\",end=\"\")                \n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred =(y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"precision_score\", precision_score(y_test, y_pred))\n",
    "        print(\"recall_score\", recall_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
